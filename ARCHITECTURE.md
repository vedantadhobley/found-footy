# Found Footy - Architecture Guide

**Temporal.io orchestration with 4-collection MongoDB architecture**

## üéØ Core Concept

**4-Collection Design with fixtures_live for Safe Comparison**

Raw API data is stored in `fixtures_live` (temporary, overwritten each poll) for comparison, while `fixtures_active` contains enhanced events that are **never overwritten** - only updated in-place by debounce job.

**Why 4 Collections?**
- **fixtures_staging**: Waiting to activate
- **fixtures_live**: Raw API data (temporary, for comparison only)
- **fixtures_active**: Enhanced events (never replaced, only updated)
- **fixtures_completed**: Archive

This prevents data loss - we can compare fresh API data against enhanced data without destroying enhancements.

---

## üìä Data Flow

```mermaid
graph TD
    A[Ingest Job<br/>Daily 00:05 UTC] -->|TBD/NS fixtures| B[fixtures_staging]
    
    B -->|Start time reached| C[Monitor: Activate<br/>Empty events array]
    C --> D[fixtures_active]
    
    E[Monitor: API Poll<br/>Every minute] -->|Filtered events<br/>with _event_id| F[fixtures_live]
    
    F -->|Compare| G{3 Trigger Cases?}
    
    G -->|Case 1: NEW| H[Debounce Job]
    G -->|Case 2: INCOMPLETE| H
    G -->|Case 3: REMOVED| H
    
    H -->|Add/Update| D
    D -->|_debounce_complete| I[Twitter Job]
    
    D -->|Status FT/AET/PEN<br/>After debounce| J[fixtures_completed]
    F -.->|Deleted| J
    
    I -->|Videos| K[MinIO S3]
    
    style F fill:#ffe6e6
    style D fill:#e6ffe6
    style J fill:#e6f3ff
```

### Comparison Logic (3 Trigger Cases)

```mermaid
flowchart LR
    Live[fixtures_live<br/>Raw API events<br/>with _event_id] --> Compare{Compare<br/>Event IDs}
    Active[fixtures_active<br/>Enhanced events<br/>with _event_id] --> Compare
    
    Compare -->|Event in live<br/>NOT in active| Case1[Case 1: NEW<br/>Trigger debounce]
    Compare -->|Event in both<br/>_debounce_complete=false| Case2[Case 2: INCOMPLETE<br/>Trigger debounce]
    Compare -->|Event in active<br/>NOT in live| Case3[Case 3: REMOVED<br/>Trigger debounce<br/>VAR disallowed]
    
    Case1 --> Debounce[Debounce Job]
    Case2 --> Debounce
    Case3 --> Debounce
```

---

## üóÑÔ∏è Collection Schemas

### fixtures_staging

Fixtures waiting to start (status TBD, NS).

```json
{
  "_id": 5000,
  "fixture": {
    "id": 5000,
    "date": "2025-11-24T15:00:00Z",
    "status": {"short": "TBD"}
  },
  "teams": {
    "home": {"id": 40, "name": "Liverpool"},
    "away": {"id": 50, "name": "Man City"}
  },
  "league": {"id": 39, "name": "Premier League"}
}
```

### fixtures_live

**Temporary storage** for raw API data. Overwritten each poll. **Filtered to Goals only**.

```json
{
  "_id": 5000,
  "stored_at": "2025-11-24T15:25:00Z",
  "fixture": {...},
  "teams": {...},
  "events": [
    {
      "player": {"id": 234, "name": "D. Szoboszlai"},
      "team": {"id": 40, "name": "Liverpool"},
      "type": "Goal",
      "detail": "Normal Goal",
      "time": {"elapsed": 23},
      "_event_id": "5000_234_23_Goal_Normal Goal"  // Generated by monitor
    }
  ]
}
```

**Key Points:**
- Only Goals stored (Normal Goal, Penalty, Own Goal)
- `_event_id` generated in monitor using `_generate_event_id()`
- Gets **overwritten** every poll cycle
- Used ONLY for comparison, not for enhancement

### fixtures_active

Enhanced fixtures. Events array **grows incrementally**, **never replaced**.

```json
{
  "_id": 5000,
  "activated_at": "2025-11-24T15:00:00Z",
  "fixture": {...},
  "teams": {...},
  "events": [
    {
      // ========== RAW API FIELDS ==========
      "player": {"id": 234, "name": "D. Szoboszlai"},
      "team": {"id": 40, "name": "Liverpool"},
      "assist": {"id": 456, "name": "M. Salah"},
      "type": "Goal",
      "detail": "Normal Goal",
      "time": {"elapsed": 23},
      
      // ========== ENHANCED FIELDS (added by debounce_job) ==========
      "_event_id": "5000_234_23_Goal_Normal Goal",
      "_stable_count": 3,
      "_debounce_complete": true,
      "_twitter_complete": false,
      "_first_seen": "2025-11-24T15:23:45Z",
      "_snapshots": [
        {"timestamp": "2025-11-24T15:23:45Z", "hash": "abc123"},
        {"timestamp": "2025-11-24T15:24:45Z", "hash": "abc123"},
        {"timestamp": "2025-11-24T15:25:45Z", "hash": "abc123"}
      ],
      "_score_before": {"home": 0, "away": 0},
      "_score_after": {"home": 1, "away": 0},
      "_scoring_team": "home",
      "_twitter_search": "Szoboszlai Liverpool"
    }
  ]
}
```

**Key Points:**
- Starts with **empty events array** when activated
- Events added by debounce job (Case 1: NEW)
- Events updated in-place (Case 2/3: INCOMPLETE/REMOVED)
- **Never replaced** - only $push and $set operations

### fixtures_completed

Archive with all enhancements intact. fixtures_live entry deleted.

```json
{
  "_id": 5000,
  "completed_at": "2025-11-24T16:50:00Z",
  "fixture": {...},
  "events": [...]  // All enhanced fields preserved
}
```

---

## üîÑ Job Details

### 1. IngestWorkflow (Daily 00:05 UTC)

**Purpose**: Fetch today's fixtures and route by status

**Activities**:
- `fetch_todays_fixtures` - Call API-Football
- `categorize_and_store_fixtures` - Route by status

**Process**:
```python
1. Fetch fixtures from API-Football for today
2. Filter to 50 tracked teams
3. Route by status:
   - TBD/NS ‚Üí fixtures_staging
   - LIVE ‚Üí fixtures_active (with empty events)
   - FT/AET/PEN ‚Üí fixtures_completed
```

**Why empty events?** MonitorWorkflow will populate events from fixtures_live comparison.

---

### 2. MonitorWorkflow (Every Minute)

**Purpose**: Activate fixtures, detect events, trigger TwitterWorkflow

**Activities** (6 total):
- `activate_fixtures` - Move staging ‚Üí active when start time reached
- `fetch_active_fixtures` - Batch fetch from API-Football
- `store_and_compare` - Filter events, store in live
- `process_fixture_events` - Pure set operations, increment stable_count
- `sync_fixture_metadata` - Keep score/status fresh
- `complete_fixture_if_ready` - Move to completed when FT/AET/PEN

**Process**:
```python
1. Activate fixtures (staging ‚Üí active with EMPTY events array)

2. Batch fetch fresh API data for ALL active fixtures

3. For each fixture:
   a. Filter events (only Goals: Normal Goal, Penalty, Own Goal)
   b. Generate _event_id for each filtered event
   c. Store in fixtures_live (overwrite previous)

4. Process events inline (pure set operations):
   a. NEW events (live - active): Add to active with stable_count=1
   b. MATCHING events (live & active): Increment stable_count
   c. REMOVED events (active - live): Mark _removed=true (VAR)
   d. stable_count >= 3: Trigger TwitterWorkflow as child

5. After processing:
   a. Check if status is FT/AET/PEN
   b. Move to fixtures_completed
```

**Event ID Format:**
```python
f"{fixture_id}_{team_id}_{player_id}_{type}_{sequence}"
# Example: "5000_40_234_Goal_1"
```

---

### 3. TwitterWorkflow (Per Stable Event)

**Purpose**: Search Twitter for event videos, trigger download

**Activities** (3 granular for retry control):
- `get_twitter_search_data` - Get search query from MongoDB
- `execute_twitter_search` - POST to Firefox automation service
- `save_twitter_results` - Save videos to MongoDB

**Process**:
```python
1. Get event from fixtures_active where:
   - _debounce_complete = true
   - _twitter_started = true (marked by workflow)

2. Use prebuilt _twitter_search field (e.g., "Salah Liverpool")

3. POST to twitter:8888/search (Firefox browser automation)

4. Save discovered videos to fixtures_active

5. If videos found AND fixture finished:
   - Trigger DownloadWorkflow as child
```

**Why 3 activities?**
- If search fails ‚Üí only retry search (not MongoDB reads)
- If save fails ‚Üí videos preserved in workflow state

---

### 4. DownloadWorkflow (Per Event with Videos)

**Purpose**: Download, deduplicate, upload videos to S3

**Activities** (5 granular for per-video retry):
- `fetch_event_data` - Get event from fixtures_active
- `download_single_video` - Download ONE video (3 retries)
- `deduplicate_videos` - MD5 hash, keep largest per hash
- `upload_single_video` - Upload ONE video to S3 (3 retries)
- `mark_download_complete` - Update MongoDB, cleanup

**Process**:
```python
1. Get event from fixtures_active with discovered_videos

2. For each video URL:
   - Download to temp directory
   - Track success/failure per video

3. Deduplicate by MD5 hash (keep largest file per hash)

4. For each unique video:
   - Upload to S3: {fixture_id}/{event_id}/{md5_hash}.mp4
   - Tag with metadata (player, team, minute)

5. Update fixtures_active:
   - Mark _download_complete = true
   - Store s3_urls array
   - Cleanup temp directory
```

**Why 5 activities?**
- If 3/5 videos succeed, those are preserved
- Failures don't lose progress
- Clear retry visibility in Temporal UI

---

## üîë Key Design Decisions

### Why fixtures_live?

**Problem**: If we overwrite fixtures_active with fresh API data, we lose all enhancement fields!

**Solution**: Store raw API data in fixtures_live (temporary), compare against fixtures_active, then update fixtures_active in-place.

### Why filter events in monitor?

**Efficiency**: Only store events we care about (Goals). No need to process substitutions, cards, etc.

### Why generate _event_id in monitor?

**Clean comparison**: Having _event_id in both live and active makes comparison trivial - just compare sets of IDs.

### Why clean iteration pattern?

**Elegance**: Build dict, pop as you process, leftovers are NEW events. No double iteration needed.

### Why hash-based stability?

**Reliability**: API might return different object order but same data. Hash catches real changes while ignoring cosmetic ones.

---

## üìù Event Enhancement Fields

| Field | Type | Purpose |
|-------|------|---------|
| `_event_id` | string | Unique ID: `{fixture_id}_{player_id}_{elapsed}_{type}_{detail}` |
| `_stable_count` | int | Consecutive unchanged polls (0-3) |
| `_debounce_complete` | bool | true when stable_count >= 3 |
| `_twitter_complete` | bool | true when videos downloaded |
| `_first_seen` | datetime | When event first appeared |
| `_snapshots` | array | History of hashes and timestamps |
| `_score_before` | object | `{"home": 0, "away": 0}` |
| `_score_after` | object | `{"home": 1, "away": 0}` |
| `_scoring_team` | string | "home" or "away" |
| `_twitter_search` | string | "{player_last_name} {team_name}" |
| `_removed` | bool | true if VAR disallowed (Case 4) |

---

## üéØ Comparison Cases (3 Trigger Debounce)

### Case 1: NEW
- **Condition**: Event in live, NOT in active
- **Action**: Add event to active with stable_count=1

### Case 2: INCOMPLETE + Hash Changed
- **Condition**: Event in both, _debounce_complete=false, hash different
- **Action**: Reset stable_count=1, update event data

### Case 3: INCOMPLETE + Hash Same
- **Condition**: Event in both, _debounce_complete=false, hash same
- **Action**: Increment stable_count, complete at 3, trigger twitter

### Case 4: REMOVED (not a trigger, handled in debounce)
- **Condition**: Event in active, NOT in live
- **Action**: Mark _removed=true (VAR disallowed goal)

---

## üöÄ Execution Flow Example

```mermaid
sequenceDiagram
    participant MW as MonitorWorkflow
    participant L as fixtures_live
    participant A as fixtures_active
    participant TW as TwitterWorkflow
    participant DW as DownloadWorkflow
    
    Note over MW: Poll 1 - 15:23:00
    MW->>L: Store event (filtered, with _event_id)
    MW->>MW: process_fixture_events: NEW event
    MW->>A: Add event (stable_count=1)
    
    Note over MW: Poll 2 - 15:24:00
    MW->>L: Store event (overwrite)
    MW->>MW: process_fixture_events: MATCHING
    MW->>A: Update (stable_count=2)
    
    Note over MW: Poll 3 - 15:25:00
    MW->>L: Store event (overwrite)
    MW->>MW: process_fixture_events: stable_count=3!
    MW->>A: Mark debounce_complete=true
    MW->>TW: Start TwitterWorkflow (child)
    
    TW->>TW: POST to twitter:8888/search
    TW->>A: Save discovered_videos
    TW->>DW: Start DownloadWorkflow (child)
    
    DW->>DW: Download + dedupe + upload to S3
    DW->>A: Mark download_complete=true
```

---

## üîç Debugging Tips

### Check fixtures_live
```javascript
// In MongoDB Express
db.fixtures_live.find()

// Should see:
// - Only Goal events
// - _event_id field present
// - stored_at timestamp
```

### Check fixtures_active
```javascript
// In MongoDB Express
db.fixtures_active.find({"events._event_id": {$exists: true}})

// Should see:
// - events array with enhancement fields
// - _stable_count progressing 1 ‚Üí 2 ‚Üí 3
// - _debounce_complete=true at stable_count=3
```

### Check comparison logic
```bash
# In worker logs
docker logs -f found-footy-worker
# Or in Temporal UI: http://localhost:4100
```

---

## üìä Collection Lifecycle

```
fixtures_staging: Hours to days (until start time)
fixtures_live: ~1 minute (overwritten each poll)
fixtures_active: ~90 minutes (fixture duration)
fixtures_completed: Forever (archive)
```

---

## üéì Summary

**4 Collections**:
1. staging: Waiting to start
2. live: Raw API (temp comparison)
3. active: Enhanced (never replaced)
4. completed: Archive

**3 Trigger Cases**:
1. NEW: Add to active
2. INCOMPLETE: Update in active
3. REMOVED: Mark removed in active

**Clean Pattern**: Dict ‚Üí Pop ‚Üí Leftovers = NEW

**No Data Loss**: live is temporary, active is sacred.
